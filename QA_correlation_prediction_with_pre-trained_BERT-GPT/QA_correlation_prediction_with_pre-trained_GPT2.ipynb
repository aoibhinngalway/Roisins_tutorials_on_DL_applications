{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd61e3a2-d8fe-4b7d-adc7-9c3658bd3f18",
   "metadata": {},
   "source": [
    "# A simple tutorial for QA correlation prediction with pre-trained language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cc0afbd-0275-4135-90d9-affca9f16b8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Róisín Luo, Colm O'Riordan (supervisor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "444981b0-eeae-47e7-bf92-77e9e53fdb75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1100cb-6c53-40fd-a364-ec1ed1466df0",
   "metadata": {},
   "source": [
    "# GPU acceleration just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7ff4056-2a2c-4448-a1de-9de3875d7cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hwacc_device_v3():\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        \n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print('CUDA memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    \n",
    "        device = torch.device('cuda')\n",
    "    # MacOS\n",
    "    elif hasattr(torch, \"backends\") and \\\n",
    "          hasattr(torch.backends, \"mps\") and \\\n",
    "          torch.backends.mps.is_available():\n",
    "                \n",
    "        device = torch.device('mps')\n",
    " \n",
    "    print(\"GPU device is: \", device)\n",
    "    \n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efac3d72-77eb-40a8-9136-3eb15be29aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device is:  mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_hwacc_device_v3()\n",
    "#device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8978b8bf-3c5c-42db-b7f9-b06b6ab39990",
   "metadata": {},
   "source": [
    "# Loading math QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e856198-fb66-4ad4-b213-a0e119fa4229",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36541"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import list_datasets\n",
    "\n",
    "datasets_list = list_datasets()\n",
    "len(datasets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a79e3676-26e6-40be-8d18-6d06eafa8c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iapp_wiki_qa_squad\n",
      "wiki_qa\n",
      "wiki_qa_ar\n",
      "wannaphong/iapp_wiki_qa_squad_oa\n",
      "sedthh/cmu_wiki_qa\n",
      "michaelthwan/wiki_qa_bart_1000row\n",
      "michaelthwan/wiki_qa_bart_10000row\n",
      "michaelthwan/oa_wiki_qa_bart_10000row\n"
     ]
    }
   ],
   "source": [
    "for ds in datasets_list:\n",
    "    if \"wiki_qa\" in ds:\n",
    "        print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d218aa5-c807-477a-b510-28a3e672c537",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wiki_qa (/Users/roisinjiaolinluo/Documents/Research/AI_Research/Roisins_Tutorials_on_DL_Applications/QA_correlation_prediction_with_pre-trained_BERT-GPT/../../Dataset_Collection/wiki_qa/default/0.1.0/d2d236b5cbdc6fbdab45d168b4d678a002e06ddea3525733a24558150585951c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383ac68f753645978eb9468835f3209e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(path = \"wiki_qa\",\n",
    "                       cache_dir = \"..\" + os.sep + \"..\" + os.sep + \"Dataset_Collection\", \n",
    "                       download_mode = \"reuse_dataset_if_exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2e6412-da05-4e52-a284-f629b46c20d7",
   "metadata": {},
   "source": [
    "## Investigating dataset shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e01d0d4-1d7e-479e-bd36-91bb14a9750c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 6165\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 2733\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 20360\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "824df8ce-8384-49d5-8ec1-1c8143d90e7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': 'Q1',\n",
       " 'question': 'how are glacier caves formed?',\n",
       " 'document_title': 'Glacier cave',\n",
       " 'answer': 'A partly submerged glacier cave on Perito Moreno Glacier .',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c18937d2-3fc1-4633-b46c-4186f15136b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20360"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106ab89c-c96e-45ea-a81b-142d97357065",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are glacier caves formed?\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76df8ac9-5f54-48d9-811c-3516a5cb1de1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A partly submerged glacier cave on Perito Moreno Glacier .\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0]['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33caf4d-6c4f-4116-98b6-14300bb84a6e",
   "metadata": {},
   "source": [
    "## Setting dataset and splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d29df82-4ce5-4347-90e6-b9e4d3b1274a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Setting format to torch or tensorflow\n",
    "dataset.set_format(type='torch', columns=['question', 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d468883-0bd8-4ecc-ad92-93539f2a011b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train_ = dataset['train']\n",
    "dataset_val_ = dataset['validation']\n",
    "dataset_test_ = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce05379-3322-40b3-83c7-7764662b8c9e",
   "metadata": {},
   "source": [
    "# Wrapping the dataset with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b4eced6-7684-4192-8014-4278c0a58893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAPredictionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 dataset, \n",
    "                 paired_sampling_prob = 0.5, \n",
    "                 random_seed = 42):\n",
    "        self.dataset = dataset\n",
    "        self.paired_sampling_prob = paired_sampling_prob\n",
    "        self.dataset_size = len(dataset)\n",
    "        \n",
    "        self.dataset_indices = list(np.arange(0, self.dataset_size))\n",
    "        \n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        q = self.dataset[index]['question']\n",
    "        a = self.dataset[index]['answer']\n",
    "        \n",
    "        MAX_LEN=512\n",
    "        #Truncate Q and A to MAX_LEN\n",
    "        #TODO.\n",
    "        \n",
    "        if np.random.rand() < self.paired_sampling_prob:\n",
    "            y = 1\n",
    "        else:\n",
    "            y = 0\n",
    "            #Resampling a 'answer'\n",
    "            new_index = int(np.random.choice(self.dataset_indices))\n",
    "            a = self.dataset[new_index]['answer']\n",
    "        \n",
    "        return (q, a), y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41e48390-0fef-4940-9037-52a9d02fb9fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = QAPredictionDataset(dataset_train_, paired_sampling_prob = 0.5)\n",
    "dataset_val = QAPredictionDataset(dataset_val_, paired_sampling_prob = 0.5)\n",
    "dataset_test = QAPredictionDataset(dataset_test_, paired_sampling_prob = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d15680e9-a9fb-472a-b9f5-e6d8a1610389",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  how are glacier caves formed?\n",
      "A:  A partly submerged glacier cave on Perito Moreno Glacier .\n",
      "Paired prob:  1\n",
      "\n",
      "Q:  how are glacier caves formed?\n",
      "A:  In modern politics, the most high profile political campaigns are focused on candidates for head of state or head of government , often a President or Prime Minister .\n",
      "Paired prob:  0\n",
      "\n",
      "Q:  how are glacier caves formed?\n",
      "A:  Recovery was an international success and was named the best selling album of 2010 worldwide, joining The Eminem Show, which was the best seller of 2002.\n",
      "Paired prob:  0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    (q,a), y = dataset_train[i]\n",
    "    print(\"Q: \", q)\n",
    "    print(\"A: \", a)\n",
    "    print(\"Paired prob: \", y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eca153a1-9ac7-456c-8d66-28ad6ea59c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0a62618-d586-4e83-8c2d-95b2a90386ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ea1e1dc-395c-47b6-9df0-d91f1b20db66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dataloader_train))\n",
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6bf4a08-24f3-4b6e-86ae-77189d7e4db8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(q,a),y = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8621ae7-f033-48da-ba36-0c650dbdbe75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd05be21-9725-44f3-9919-882e37416170",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1be76d5-04a0-467f-a076-84a4f2dd1b17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many countries are member of the eu?\n"
     ]
    }
   ],
   "source": [
    "print(q[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72330e5a-72c3-4b7a-aa34-d62b6d3407ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The EU was the recipient of the 2012 Nobel Peace Prize .\n"
     ]
    }
   ],
   "source": [
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3652c1c8-4d58-43f1-9968-911bd7622225",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b7cd17-8874-43d2-9be5-b708625e1c80",
   "metadata": {},
   "source": [
    "# Loading pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02023a77-a341-44c1-8e83-886a2ac0784b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt = GPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042bf7e0-9014-41e2-8629-db081adac467",
   "metadata": {},
   "source": [
    "## Testing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7cc67ed-5ba8-43d6-8cf7-79797da53342",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('how many countries are member of the eu?', 'where are poison dart frog seen', 'how many missions has the us sent to mars', 'what is vat tax?', 'when does college football training camp start', 'who wrote the song a little more country than that>', 'how many countries have english as an official language', 'who was in great britain before the anglo-saxons ?')\n"
     ]
    }
   ],
   "source": [
    "#a batch of text\n",
    "text = q\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bc1b03b-9bae-48b0-a220-edd1e0feae57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[4919,  867, 2678,  389, 2888,  286,  262,  304,   84,   30]],\n",
      "       device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "# Tokenized input\n",
    "inputs = tokenizer(text[0], return_tensors='pt')\n",
    "\n",
    "#sending to GPU\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0195e089-bbcc-4c74-86ab-8f824f0d303f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sending model to GPU if possible\n",
    "gpt.to(device)\n",
    "\n",
    "#get embeddings.\n",
    "with torch.no_grad():\n",
    "    outputs = gpt(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e986186a-5a3c-4598-a8bf-c099a77d04c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_hidden_state = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5010e4e-924b-4366-a17c-08d177ef0f21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "721407c1-2794-488b-9eed-c245d101e33c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Unlike BERT we use [CLS] as context.\n",
    "#In GPT, we simply add all token representations.\n",
    "\n",
    "context = torch.sum(last_hidden_state, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a618efd-8f3e-4009-bc3e-841430fa8563",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35e7c8b4-4444-4e9c-a06b-13f74274d6ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We use this position as embedding.\n",
    "context[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f34c8dd-3478-4342-a997-648c7f8f57c0",
   "metadata": {},
   "source": [
    "# Building correlation prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4eaa25bd-d02a-42bc-a48f-1c1e792c041f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class QACorrelationPredictionModel(nn.Module):\n",
    "    def __init__(self, device = torch.device(\"cpu\")):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "  \n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.gpt = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "\n",
    "        #correlation prediction head.\n",
    "        self.pred_head = nn.Sequential(\n",
    "                    #nn.Dropout(p = 0.1),\n",
    "                    \n",
    "                    #Because we add all token representations as context.\n",
    "                    #So we need to re-center the representations statistically.\n",
    "                    #\n",
    "                    #BUT for NLP task, we DONT use batchnorm. instead SHOULD use layernorm.\n",
    "                    nn.LayerNorm(normalized_shape = (768*2)), \n",
    "                    nn.Linear(in_features = 768*2, out_features = 100, bias = True),\n",
    "                    nn.ReLU(),\n",
    "            \n",
    "                    nn.Linear(in_features = 100, out_features = 1, bias = True),\n",
    "                    nn.Sigmoid(),\n",
    "                    )\n",
    "        #self.pred_head.to(device)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        self.gpt.eval()\n",
    "        \n",
    "        #A batch consists of ((q,a), y), in which\n",
    "        #q and q are lists, y an array.\n",
    "\n",
    "        #get embeddings\n",
    "        qa_embed = []\n",
    "        \n",
    "        (q_list, a_list), y = batch\n",
    "        \n",
    "        for q, a in zip(q_list, a_list):\n",
    "            \n",
    "            #print()\n",
    "            #print(\"Q: \", q)\n",
    "            #print(\"A: \", a)\n",
    "            \n",
    "            # Tokenized Q and A\n",
    "            inputs_q = self.tokenizer(q, return_tensors='pt')\n",
    "            inputs_a = self.tokenizer(a, return_tensors='pt')\n",
    "            \n",
    "            inputs_q = inputs_q.to(self.device)\n",
    "            inputs_a = inputs_a.to(self.device)\n",
    "            \n",
    "            #get context vectors.\n",
    "            with torch.no_grad():\n",
    "                outputs_q = self.gpt(**inputs_q)\n",
    "                outputs_a = self.gpt(**inputs_q)\n",
    "      \n",
    "            last_hidden_state_q = outputs_q.last_hidden_state\n",
    "            last_hidden_state_a = outputs_a.last_hidden_state\n",
    "            #Unlike BERT we use [CLS] as context.\n",
    "            #In GPT, we simply add all token representations.\n",
    "\n",
    "            context_q = torch.sum(last_hidden_state_q, dim = 1)\n",
    "            context_a = torch.sum(last_hidden_state_a, dim = 1)\n",
    "            \n",
    "            #print(\"last_hidden_state_q.shape = \", last_hidden_state_q.shape)\n",
    "            context_q = context_q.squeeze(0)\n",
    "            context_a = context_a.squeeze(0)\n",
    "        \n",
    "            #print()\n",
    "            #print(\"q_embed.shape = \", q_embed.shape)\n",
    "            #print(\"a_embed.shape = \", a_embed.shape)\n",
    "        \n",
    "            #Concatenate two 768 into 768*2\n",
    "            embed = torch.cat([context_q, context_a])\n",
    "            #print(\"embed.shape = \", embed.shape)\n",
    "            \n",
    "            qa_embed.append(embed)\n",
    "            \n",
    "        qa_embed = torch.stack(qa_embed)\n",
    "        #print(\"qa_embed.shape = \", qa_embed.shape)\n",
    "        \n",
    "        #stop gradients.\n",
    "        qa_embed = qa_embed.detach()\n",
    "        \n",
    "        probs = self.pred_head(qa_embed)\n",
    "        probs = probs.squeeze(1)\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95b25720-d943-45a1-8261-ea9246567d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2336024c-a65b-40ea-b75d-ef50edca111b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q,a),labels = batch\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77444dc8-0058-4fab-b440-355c1ea11f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#testing the model simply.\n",
    "model = QACorrelationPredictionModel(device = device)\n",
    "model.to(device)\n",
    "batch = next(iter(dataloader_train))\n",
    "probs = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6251de52-8749-4251-b136-b2f1c2072ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df4cb7c0-89d2-48e3-9803-cb020f83075f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4834, 0.4755, 0.4743, 0.4775, 0.4827, 0.4804, 0.4751, 0.4844],\n",
       "       device='mps:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d198337-ada1-4c9f-9d3b-f050a9bd75c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c6c047fd-2913-4e32-baeb-042a9007c99d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(probs >= 0.5, dtype = torch.float).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "181f5ff2-a7d3-47ad-80b9-1ca24b7d9d12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(_, _), labels = batch\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a2cd870-5455-4c25-aa5d-9d4ab9d72cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7990bd6c-e5db-47e4-9452-0f36cc7d381d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "757bee96-a099-4f77-94f7-4786f5e49336",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6994, device='mps:0', grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "#sending to GPU.\n",
    "probs = probs.to(device)\n",
    "labels = labels.float().to(device)\n",
    "\n",
    "criterion(probs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd427fa-a78d-4d6c-9103-0793f72199ea",
   "metadata": {},
   "source": [
    "# Training one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9ded1e1c-b804-4db8-845e-5c4914bbbcc0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#from IPython.display import display, clear_output\n",
    "from IPython import display\n",
    "\n",
    "def train_one_epoch(\n",
    "          model, \n",
    "          device, \n",
    "          dataloader, \n",
    "          optimizer, \n",
    "          criterion,\n",
    "          epoch,\n",
    "          max_batches = None):\n",
    "    \n",
    "    # Enable gradient computing\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    if max_batches is None:\n",
    "        max_batches = len(dataloader)\n",
    "    \n",
    "    #some statistics\n",
    "    \n",
    "    #averaged loss in current epoch.\n",
    "    epoch_loss = 0.0\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    #accuracy in current epoch\n",
    "    batch_accuracy = 0.0\n",
    "    #accuracy in current batch\n",
    "    epoch_accuracy = 0.0\n",
    "    \n",
    "    #how many samples predicted correct.\n",
    "    epoch_corrects = 0.0\n",
    "    #how many samples trained in this epoch\n",
    "    epoch_total = 0.0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader, 1):\n",
    "        \n",
    "        (q,a), labels_ = batch\n",
    "        \n",
    "        labels = labels_.float()\n",
    "        #sending labels to GPU if possible\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #predictions.\n",
    "        preds = model(batch)\n",
    "        \n",
    "        #computing BCE\n",
    "        loss = criterion(preds, labels)\n",
    "           \n",
    "        #computing gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        #optimizing the classifier, Notice: the GPT is fixed.\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        #computing accuracy in a batch\n",
    "        # torch.max() returns values, indices\n",
    "        preds_ = (preds >= 0.5).int().cpu().data\n",
    "        #batch_accuracy = torch.mean(preds > 0.5, dtype = torch.float).detach().cpu()\n",
    "        #batch_accuracy = torch.mean(preds_.float()).detach().cpu()\n",
    "        \n",
    "        #computing the total loss and average loss in one epoch\n",
    "        total_loss += loss.detach().cpu().numpy()\n",
    "        epoch_loss = total_loss / batch_idx\n",
    "        \n",
    "        #computing the correct and total samples\n",
    "        batch_corrects = torch.sum(labels_.cpu().data == preds_, dtype = torch.int)\n",
    "        batch_accuracy = batch_corrects / len(labels_)\n",
    "        epoch_corrects += batch_corrects\n",
    "        epoch_total += len(labels_)\n",
    "        epoch_accuracy = epoch_corrects / epoch_total         \n",
    "\n",
    "        #Updating training displays.\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        display.display('Epoch {} [{}/{} ({:.0f}%)]'.format(\n",
    "                    epoch, batch_idx, \n",
    "                    len(dataloader), \n",
    "                    100. * (batch_idx / len(dataloader))))\n",
    "        \n",
    "        display.display('* batch accuracy {:.2f}% epoch accuracy {:.2f}%'.format(\n",
    "                    100. * batch_accuracy, 100. * epoch_accuracy))\n",
    "        \n",
    "        display.display('* batch loss {:.6f} epoch loss {:.6f}'.format(\n",
    "                    loss.item(), epoch_loss))\n",
    "        display.display('* batch_corrects {}'.format(batch_corrects))\n",
    "        \n",
    "        if batch_idx > max_batches:\n",
    "            break\n",
    "    \n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "15f77096-c2c2-4b3a-85f7-7061cd688abd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = QACorrelationPredictionModel(device = device)\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8ad05155-dc6f-4c5b-b02e-ad6f188914ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Epoch 1 [165/319 (52%)]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* batch accuracy 50.00% epoch accuracy 50.09%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* batch loss 0.692932 epoch loss 0.698094'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* batch_corrects 32'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#Loss function\u001b[39;00m\n\u001b[1;32m     17\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[0;32m---> 19\u001b[0m epoch_loss, epoch_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m          \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmax_batches\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[66], line 49\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, device, dataloader, optimizer, criterion, epoch, max_batches)\u001b[0m\n\u001b[1;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#predictions.\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#computing BCE\u001b[39;00m\n\u001b[1;32m     52\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(preds, labels)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[54], line 60\u001b[0m, in \u001b[0;36mQACorrelationPredictionModel.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#get context vectors.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 60\u001b[0m     outputs_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs_q\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     outputs_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs_q)\n\u001b[1;32m     63\u001b[0m last_hidden_state_q \u001b[38;5;241m=\u001b[39m outputs_q\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:900\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    890\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    891\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    892\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 900\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:202\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull([], mask_value, dtype\u001b[38;5;241m=\u001b[39mattn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mto(attn_weights\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 202\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m+\u001b[39m attention_mask\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "#for language models, Adam is a good option.The learning rate \n",
    "#typically less than 0.001 for stabability.\n",
    "optimizer = torch.optim.Adam(\n",
    "                        model.pred_head.parameters(), \n",
    "                        lr = learning_rate,\n",
    "                        #momentum = 0.9, \n",
    "                        #weight_decay = 5e-4\n",
    "                      )\n",
    "\n",
    "#Loss function\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "epoch_loss, epoch_accuracy = train_one_epoch(\n",
    "          model, \n",
    "          device, \n",
    "          dataloader_train, \n",
    "          optimizer, \n",
    "          criterion,\n",
    "          epoch = 1,\n",
    "          max_batches = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc7cfc-35bf-4677-9711-d3a1a8b8c098",
   "metadata": {},
   "source": [
    "# Save/load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "28bc1e9e-4077-4620-98e2-64c4e26cff48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model(model, model_path):\n",
    "    \n",
    "    save_path = os.path.normpath(os.path.dirname(model_path)).rstrip(os.path.sep)\n",
    "        \n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    print(\"Save model weights to: \", model_path)\n",
    "    torch.save(model.state_dict(), model_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4e23da03-e31a-4c48-8c7b-19149cd2c80d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model weights to:  models/qa_model_distilbert-base-uncased.pth\n"
     ]
    }
   ],
   "source": [
    "save_model(model, \"models/qa_model_gpt2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "942659e0-8043-4359-b9f2-3d75e67f5eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model_path, device):\n",
    "    model = QACorrelationPredictionModel(device)\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        #re-loading\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device)) \n",
    "        print(\"Loaded model weights from: \", model_path)\n",
    "    else:\n",
    "        print(\"Model weights not found.\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "20fbb4cd-2d70-4ef3-946d-54f4ab25b3cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model weights from:  models/qa_model_distilbert-base-uncased.pth\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"models/qa_model_gpt2.pth\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd2a73-7485-475b-b343-126dc2636e3f",
   "metadata": {},
   "source": [
    "# Complete training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "150a9ad5-0d71-418b-bdf6-437b245bc983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, \n",
    "          device, \n",
    "          dataloader, \n",
    "          optimizer,\n",
    "          criterion,\n",
    "          epochs,\n",
    "          scheduler = None):\n",
    "    \n",
    "    #if model_path is not None and not os.path.exists(model_path):\n",
    "    #    os.makedirs(model_path)\n",
    "    \n",
    "    loss_hist = []\n",
    "    accuracy_hist = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        \n",
    "        epoch_loss, epoch_accuracy = train_one_epoch(\n",
    "          model, \n",
    "          device, \n",
    "          dataloader, \n",
    "          optimizer, \n",
    "          criterion,\n",
    "          epoch,\n",
    "          max_batches = None)\n",
    "    \n",
    "        if scheduler:\n",
    "            #adjusting LR is necessary\n",
    "            scheduler.step()\n",
    "            \n",
    "        loss_hist.append(epoch_loss)\n",
    "        accuracy_hist.append(epoch_accuracy)\n",
    "   \n",
    "    return loss_hist, laccuracy_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "76733541-38b9-4f41-8c09-440fbd24f5d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "#for language models, Adam is a good option.The learning rate \n",
    "#typically less than 0.001 for stabability.\n",
    "optimizer = torch.optim.Adam(\n",
    "                        model.pred_head.parameters(), \n",
    "                        lr = learning_rate,\n",
    "                        #momentum = 0.9, \n",
    "                        #weight_decay = 5e-4\n",
    "                      )\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "#                                                step_size = 10, #dropping learning-rate every 10 steps. \n",
    "#                                                gamma = 0.1)\n",
    "scheduler = None\n",
    "\n",
    "#Loss function\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "39aec210-b4ca-494b-9cfb-3361fab4606b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Epoch 1 [3/319 (1%)]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* batch accuracy 85.94% epoch accuracy 85.94%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* batch loss 0.331381 epoch loss 0.336008'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* batch_corrects 55'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss_hist, laccuracy_hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m          \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, dataloader, optimizer, criterion, epochs, scheduler)\u001b[0m\n\u001b[1;32m     13\u001b[0m accuracy_hist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     epoch_loss, epoch_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_batches\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scheduler:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m#adjusting LR is necessary\u001b[39;00m\n\u001b[1;32m     28\u001b[0m         scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[50], line 49\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, device, dataloader, optimizer, criterion, epoch, max_batches)\u001b[0m\n\u001b[1;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#predictions.\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#computing BCE\u001b[39;00m\n\u001b[1;32m     52\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(preds, labels)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[38], line 56\u001b[0m, in \u001b[0;36mQACorrelationPredictionModel.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# We only need last layer output the CLS position as embedding.!\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 56\u001b[0m     outputs_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask_q\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     outputs_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(input_ids_a, attention_mask_a)\n\u001b[1;32m     59\u001b[0m last_hidden_state_q \u001b[38;5;241m=\u001b[39m outputs_q\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:583\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    579\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    581\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids, inputs_embeds)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:359\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    357\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 359\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:295\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    304\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:233\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[0;32m--> 233\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    234\u001b[0m context \u001b[38;5;241m=\u001b[39m unshape(context)  \u001b[38;5;66;03m# (bs, q_length, dim)\u001b[39;00m\n\u001b[1;32m    235\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_lin(context)  \u001b[38;5;66;03m# (bs, q_length, dim)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_hist, laccuracy_hist = train(model, \n",
    "          device, \n",
    "          dataloader_train, \n",
    "          optimizer,\n",
    "          criterion,\n",
    "          epochs = 50,\n",
    "          scheduler = scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb1961-76ce-40dc-9f27-ccf265940140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c3326-fc90-431b-9124-320142289d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(laccuracy_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d56cdd4-37a9-4aa4-8730-11af6336ebf8",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d8f2622-443f-4373-84c0-8cd3ae74be58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, \n",
    "             device, \n",
    "             dataloader,\n",
    "             max_batches = None):\n",
    "    \n",
    "    # Disable gradient computing\n",
    "    model.eval()\n",
    "    \n",
    "    if max_batches is None:\n",
    "        max_batches = len(dataloader)\n",
    "    \n",
    "    batch_accuracy = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    \n",
    "    total_corrects = 0.0\n",
    "    total_entries = 0.0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader, 1):\n",
    "        \n",
    "        (q,a), labels = batch\n",
    "        \n",
    "        \n",
    "        #no need to track gradients\n",
    "        with torch.no_grad():\n",
    "            preds = model(batch)\n",
    "                        \n",
    "        #computing accuracy in a batch\n",
    "        preds_ = (preds >= 0.5).int().cpu().data\n",
    "        \n",
    "        #computing the correct and total samples\n",
    "        batch_corrects = torch.sum(labels.cpu().data == preds_, dtype = torch.int)\n",
    "        batch_accuracy = batch_corrects / len(labels)\n",
    "        \n",
    "        total_corrects += batch_corrects\n",
    "        total_entries += len(labels)\n",
    "        total_accuracy = total_corrects / total_entries         \n",
    "\n",
    "        #Updating training displays.\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        display.display('Evaluation: [{}/{} ({:.0f}%)]'.format(\n",
    "                    batch_idx, \n",
    "                    len(dataloader), \n",
    "                    100. * (batch_idx / len(dataloader))))\n",
    "        \n",
    "        display.display('* batch accuracy {:.2f}% total accuracy {:.2f}%'.format(\n",
    "                    100. * batch_accuracy, 100. * total_accuracy))\n",
    "        \n",
    "        display.display('* total_corrects {} total_entries {}'.format(total_corrects, total_entries))\n",
    "        \n",
    "        if batch_idx >= max_batches:\n",
    "            break\n",
    "                \n",
    "    return total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed18838f-b715-479e-a545-9edfa9d2ce65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Evaluation: [10/97 (10%)]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* batch accuracy 87.50% total accuracy 82.03%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* total_corrects 525.0 total_entries 640.0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8203)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "total_accuracy = evaluate(model, \n",
    "             device, \n",
    "             dataloader_test,\n",
    "             max_batches = 10)\n",
    "\n",
    "print(total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2107ab6d-2e3e-4454-8f2d-3e81c5fc2fdb",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cab38e0d-8fde-4d10-a30d-c36424b02f29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "23c9ac3d-0d63-4d93-b198-b84bb84dc345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "590cf97d-0f17-4229-bbfd-aa670642e979",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(q,a),labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ec930129-ff80-4a54-bdfa-299c3db87e7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4a9c0411-8c4c-4d04-aa8e-5db8e2563380",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0273e-02, 3.2407e-02, 3.5677e-01, 4.7347e-03, 2.5171e-01, 9.0781e-01,\n",
       "        1.6453e-03, 1.4629e-01, 5.2216e-01, 9.1657e-01, 2.1846e-02, 9.8038e-02,\n",
       "        6.6185e-01, 3.4565e-04, 7.9286e-02, 3.0783e-01, 1.4309e-01, 8.7267e-01,\n",
       "        1.8307e-03, 1.2167e-01, 2.4902e-01, 7.6512e-01, 5.2146e-01, 9.2054e-01,\n",
       "        4.6154e-01, 8.7516e-01, 1.1504e-02, 9.7972e-01, 8.2628e-01, 8.4387e-01,\n",
       "        2.4516e-02, 8.2525e-01, 2.9915e-01, 9.1337e-01, 6.8162e-01, 9.4618e-01,\n",
       "        7.4872e-01, 1.8747e-01, 3.7354e-01, 8.1918e-01, 8.5369e-01, 8.8730e-01,\n",
       "        7.3736e-01, 4.4461e-03, 7.1582e-01, 6.9760e-01, 3.3097e-01, 7.3234e-01,\n",
       "        9.0394e-02, 7.5479e-03, 1.1329e-01, 5.9142e-01, 7.5703e-01, 4.7870e-02,\n",
       "        3.8350e-03, 3.4489e-02, 8.4882e-01, 8.2115e-01, 9.1722e-01, 7.1167e-03,\n",
       "        6.7794e-04, 5.1792e-01, 8.1934e-01, 4.1382e-01, 3.2099e-02, 7.0505e-01,\n",
       "        1.7920e-02, 5.4763e-02, 9.2899e-01, 7.5020e-01, 9.6094e-01, 5.8948e-01,\n",
       "        7.6573e-01, 7.5728e-01, 6.5120e-01, 9.2896e-01, 4.5136e-02, 9.4404e-01,\n",
       "        5.3019e-01, 2.0179e-01, 5.5576e-04, 6.0122e-01, 8.8614e-01, 6.1427e-01,\n",
       "        8.6039e-01, 9.0616e-01, 5.9285e-01, 1.3244e-02, 1.1419e-02, 6.5591e-01,\n",
       "        7.9133e-01, 5.6049e-01, 8.4609e-01, 7.8564e-01, 9.4769e-02, 7.5266e-01,\n",
       "        1.0318e-03, 3.2372e-03, 9.7523e-01, 1.8781e-02], device='mps:0',\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction probability.\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "23be43b2-a5d5-48ce-bbf9-35098f3659fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "        0, 0, 1, 0])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba0c5665-7e2b-48c7-ad60-dd3703cacf33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "        0, 0, 1, 0], device='mps:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds > 0.5).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "adef5257-2620-4cc7-bad1-3c3e979f51ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8100000023841858"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#average accuracy\n",
    "torch.mean((preds > 0.5).int().cpu() == labels.int().cpu(), dtype = torch.float).cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bdf1a1-df05-4ee4-bfab-dc4828558014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132665a2-ef09-440e-a2a6-fd48e68e7c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
