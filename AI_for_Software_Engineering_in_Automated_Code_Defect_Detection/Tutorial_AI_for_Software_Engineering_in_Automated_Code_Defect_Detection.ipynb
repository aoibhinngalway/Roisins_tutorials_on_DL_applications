{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for Software Engineering: Automated Source Code Defect Detection with CodeBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Róisín Luo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU acceleration just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hwacc_device_v3():\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        \n",
    "        print('# of CUDA devices:', torch.cuda.device_count())\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print('CUDA memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    \n",
    "        device = torch.device('cuda')\n",
    "    # MacOS\n",
    "    elif hasattr(torch, \"backends\") and \\\n",
    "          hasattr(torch.backends, \"mps\") and \\\n",
    "          torch.backends.mps.is_available():\n",
    "                \n",
    "        device = torch.device('mps')\n",
    " \n",
    "    print(\"GPU device is: \", device)\n",
    "    \n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-PCIE-16GB\n",
      "CUDA memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "GPU device is:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_hwacc_device_v3()\n",
    "#device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import list_datasets\n",
    "\n",
    "datasets_list = list_datasets()\n",
    "len(datasets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ds in datasets_list:\n",
    "    if \"defect\" in ds:\n",
    "        print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"dataset_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(cache_dir + os.sep + \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(path = \"semeru/code-code-DefectDetection\",\n",
    "                       cache_dir = cache_dir, \n",
    "                       download_mode = \"reuse_dataset_if_exists\")\n",
    "dataset.save_to_disk('dataset_cache/code-code-DefectDetection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"dataset_cache/code-code-DefectDetection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating dataset shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['project', 'commit_id', 'target', 'func', 'idx'],\n",
       "        num_rows: 21854\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['project', 'commit_id', 'target', 'func', 'idx'],\n",
       "        num_rows: 2732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['project', 'commit_id', 'target', 'func', 'idx'],\n",
       "        num_rows: 2732\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'project': 'FFmpeg',\n",
       " 'commit_id': '973b1a6b9070e2bf17d17568cbaf4043ce931f51',\n",
       " 'target': 0,\n",
       " 'func': 'static av_cold int vdadec_init(AVCodecContext *avctx)\\n\\n{\\n\\n    VDADecoderContext *ctx = avctx->priv_data;\\n\\n    struct vda_context *vda_ctx = &ctx->vda_ctx;\\n\\n    OSStatus status;\\n\\n    int ret;\\n\\n\\n\\n    ctx->h264_initialized = 0;\\n\\n\\n\\n    /* init pix_fmts of codec */\\n\\n    if (!ff_h264_vda_decoder.pix_fmts) {\\n\\n        if (kCFCoreFoundationVersionNumber < kCFCoreFoundationVersionNumber10_7)\\n\\n            ff_h264_vda_decoder.pix_fmts = vda_pixfmts_prior_10_7;\\n\\n        else\\n\\n            ff_h264_vda_decoder.pix_fmts = vda_pixfmts;\\n\\n    }\\n\\n\\n\\n    /* init vda */\\n\\n    memset(vda_ctx, 0, sizeof(struct vda_context));\\n\\n    vda_ctx->width = avctx->width;\\n\\n    vda_ctx->height = avctx->height;\\n\\n    vda_ctx->format = \\'avc1\\';\\n\\n    vda_ctx->use_sync_decoding = 1;\\n\\n    vda_ctx->use_ref_buffer = 1;\\n\\n    ctx->pix_fmt = avctx->get_format(avctx, avctx->codec->pix_fmts);\\n\\n    switch (ctx->pix_fmt) {\\n\\n    case AV_PIX_FMT_UYVY422:\\n\\n        vda_ctx->cv_pix_fmt_type = \\'2vuy\\';\\n\\n        break;\\n\\n    case AV_PIX_FMT_YUYV422:\\n\\n        vda_ctx->cv_pix_fmt_type = \\'yuvs\\';\\n\\n        break;\\n\\n    case AV_PIX_FMT_NV12:\\n\\n        vda_ctx->cv_pix_fmt_type = \\'420v\\';\\n\\n        break;\\n\\n    case AV_PIX_FMT_YUV420P:\\n\\n        vda_ctx->cv_pix_fmt_type = \\'y420\\';\\n\\n        break;\\n\\n    default:\\n\\n        av_log(avctx, AV_LOG_ERROR, \"Unsupported pixel format: %d\\\\n\", avctx->pix_fmt);\\n\\n        goto failed;\\n\\n    }\\n\\n    status = ff_vda_create_decoder(vda_ctx,\\n\\n                                   avctx->extradata, avctx->extradata_size);\\n\\n    if (status != kVDADecoderNoErr) {\\n\\n        av_log(avctx, AV_LOG_ERROR,\\n\\n                \"Failed to init VDA decoder: %d.\\\\n\", status);\\n\\n        goto failed;\\n\\n    }\\n\\n    avctx->hwaccel_context = vda_ctx;\\n\\n\\n\\n    /* changes callback functions */\\n\\n    avctx->get_format = get_format;\\n\\n    avctx->get_buffer2 = get_buffer2;\\n\\n#if FF_API_GET_BUFFER\\n\\n    // force the old get_buffer to be empty\\n\\n    avctx->get_buffer = NULL;\\n\\n#endif\\n\\n\\n\\n    /* init H.264 decoder */\\n\\n    ret = ff_h264_decoder.init(avctx);\\n\\n    if (ret < 0) {\\n\\n        av_log(avctx, AV_LOG_ERROR, \"Failed to open H.264 decoder.\\\\n\");\\n\\n        goto failed;\\n\\n    }\\n\\n    ctx->h264_initialized = 1;\\n\\n\\n\\n    return 0;\\n\\n\\n\\nfailed:\\n\\n    vdadec_close(avctx);\\n\\n    return -1;\\n\\n}\\n',\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static av_cold int vdadec_init(AVCodecContext *avctx)\n",
      "\n",
      "{\n",
      "\n",
      "    VDADecoderContext *ctx = avctx->priv_data;\n",
      "\n",
      "    struct vda_context *vda_ctx = &ctx->vda_ctx;\n",
      "\n",
      "    OSStatus status;\n",
      "\n",
      "    int ret;\n",
      "\n",
      "\n",
      "\n",
      "    ctx->h264_initialized = 0;\n",
      "\n",
      "\n",
      "\n",
      "    /* init pix_fmts of codec */\n",
      "\n",
      "    if (!ff_h264_vda_decoder.pix_fmts) {\n",
      "\n",
      "        if (kCFCoreFoundationVersionNumber < kCFCoreFoundationVersionNumber10_7)\n",
      "\n",
      "            ff_h264_vda_decoder.pix_fmts = vda_pixfmts_prior_10_7;\n",
      "\n",
      "        else\n",
      "\n",
      "            ff_h264_vda_decoder.pix_fmts = vda_pixfmts;\n",
      "\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "    /* init vda */\n",
      "\n",
      "    memset(vda_ctx, 0, sizeof(struct vda_context));\n",
      "\n",
      "    vda_ctx->width = avctx->width;\n",
      "\n",
      "    vda_ctx->height = avctx->height;\n",
      "\n",
      "    vda_ctx->format = 'avc1';\n",
      "\n",
      "    vda_ctx->use_sync_decoding = 1;\n",
      "\n",
      "    vda_ctx->use_ref_buffer = 1;\n",
      "\n",
      "    ctx->pix_fmt = avctx->get_format(avctx, avctx->codec->pix_fmts);\n",
      "\n",
      "    switch (ctx->pix_fmt) {\n",
      "\n",
      "    case AV_PIX_FMT_UYVY422:\n",
      "\n",
      "        vda_ctx->cv_pix_fmt_type = '2vuy';\n",
      "\n",
      "        break;\n",
      "\n",
      "    case AV_PIX_FMT_YUYV422:\n",
      "\n",
      "        vda_ctx->cv_pix_fmt_type = 'yuvs';\n",
      "\n",
      "        break;\n",
      "\n",
      "    case AV_PIX_FMT_NV12:\n",
      "\n",
      "        vda_ctx->cv_pix_fmt_type = '420v';\n",
      "\n",
      "        break;\n",
      "\n",
      "    case AV_PIX_FMT_YUV420P:\n",
      "\n",
      "        vda_ctx->cv_pix_fmt_type = 'y420';\n",
      "\n",
      "        break;\n",
      "\n",
      "    default:\n",
      "\n",
      "        av_log(avctx, AV_LOG_ERROR, \"Unsupported pixel format: %d\\n\", avctx->pix_fmt);\n",
      "\n",
      "        goto failed;\n",
      "\n",
      "    }\n",
      "\n",
      "    status = ff_vda_create_decoder(vda_ctx,\n",
      "\n",
      "                                   avctx->extradata, avctx->extradata_size);\n",
      "\n",
      "    if (status != kVDADecoderNoErr) {\n",
      "\n",
      "        av_log(avctx, AV_LOG_ERROR,\n",
      "\n",
      "                \"Failed to init VDA decoder: %d.\\n\", status);\n",
      "\n",
      "        goto failed;\n",
      "\n",
      "    }\n",
      "\n",
      "    avctx->hwaccel_context = vda_ctx;\n",
      "\n",
      "\n",
      "\n",
      "    /* changes callback functions */\n",
      "\n",
      "    avctx->get_format = get_format;\n",
      "\n",
      "    avctx->get_buffer2 = get_buffer2;\n",
      "\n",
      "#if FF_API_GET_BUFFER\n",
      "\n",
      "    // force the old get_buffer to be empty\n",
      "\n",
      "    avctx->get_buffer = NULL;\n",
      "\n",
      "#endif\n",
      "\n",
      "\n",
      "\n",
      "    /* init H.264 decoder */\n",
      "\n",
      "    ret = ff_h264_decoder.init(avctx);\n",
      "\n",
      "    if (ret < 0) {\n",
      "\n",
      "        av_log(avctx, AV_LOG_ERROR, \"Failed to open H.264 decoder.\\n\");\n",
      "\n",
      "        goto failed;\n",
      "\n",
      "    }\n",
      "\n",
      "    ctx->h264_initialized = 1;\n",
      "\n",
      "\n",
      "\n",
      "    return 0;\n",
      "\n",
      "\n",
      "\n",
      "failed:\n",
      "\n",
      "    vdadec_close(avctx);\n",
      "\n",
      "    return -1;\n",
      "\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0]['func'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting dataset and splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Setting format to torch or tensorflow\n",
    "dataset.set_format(type='torch', columns=['func', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train_ = dataset['train']\n",
    "dataset_val_ = dataset['validation']\n",
    "dataset_test_ = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_[0]['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21854"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "\n",
    "class CodeDefectDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 dataset, \n",
    "                 random_seed = 42):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.dataset_size = len(dataset)\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "        \n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        code = self.dataset[index]['func']\n",
    "        target = self.dataset[index]['target']\n",
    "        \n",
    "        #BERT has maximum 512 token limit, we\n",
    "        #split code into several code blocks.\n",
    "        BLOCK_MAX_LEN = 500\n",
    "        \n",
    "        tokens = self.tokenizer.tokenize(code)\n",
    "        tokens = [self.tokenizer.cls_token] + tokens + [self.tokenizer.eos_token]\n",
    "        \n",
    "        num_tokens = len(tokens)\n",
    "        num_block_tokens = num_tokens // BLOCK_MAX_LEN\n",
    "        \n",
    "        if num_tokens % BLOCK_MAX_LEN > 0:\n",
    "            num_block_tokens += 1\n",
    "        \n",
    "        block_tokens_list = []\n",
    "        block_token_ids_list = []\n",
    "        residual = num_tokens\n",
    "        for n in range(num_block_tokens):\n",
    "            if residual >= BLOCK_MAX_LEN:\n",
    "                token_len = BLOCK_MAX_LEN\n",
    "            else:\n",
    "                token_len = residual\n",
    "            \n",
    "            residual -= token_len\n",
    "       \n",
    "            block_tokens = tokens[n*BLOCK_MAX_LEN:n*BLOCK_MAX_LEN + token_len]\n",
    "\n",
    "            block_tokens_list.append(block_tokens)\n",
    "\n",
    "            block_token_ids = self.tokenizer.convert_tokens_to_ids(block_tokens)\n",
    "            \n",
    "            block_token_ids_list.append(block_token_ids)\n",
    "        \n",
    "            if residual == 0:\n",
    "                break\n",
    "                \n",
    "        assert sum([len(t) for t in block_tokens_list]) == len(tokens)\n",
    "        assert sum([len(t) for t in block_token_ids_list]) == len(tokens)\n",
    "        \n",
    "        return (code, tokens, block_tokens_list, block_token_ids_list), target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = CodeDefectDataset(dataset_train_)\n",
    "dataset_val = CodeDefectDataset(dataset_val_)\n",
    "dataset_test = CodeDefectDataset(dataset_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens):  1123\n",
      "len(block_tokens_list):  3\n",
      "len(block_token_ids_list):  3\n",
      "target:  tensor(0)\n",
      "\n",
      "len(tokens):  20598\n",
      "len(block_tokens_list):  42\n",
      "len(block_token_ids_list):  42\n",
      "target:  tensor(0)\n",
      "\n",
      "len(tokens):  277\n",
      "len(block_tokens_list):  1\n",
      "len(block_token_ids_list):  1\n",
      "target:  tensor(0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    (code, tokens, block_tokens_list, block_token_ids_list), target = dataset_train[i]\n",
    "    print(\"len(tokens): \", len(tokens))\n",
    "    print(\"len(block_tokens_list): \", len(block_tokens_list))\n",
    "    print(\"len(block_token_ids_list): \", len(block_token_ids_list))\n",
    "    print(\"target: \", target)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#making data into batch\n",
    "def dataset_collate_fn(batch):\n",
    "    target_batch = []\n",
    "    code_batch = []\n",
    "    tokens_batch = []\n",
    "    block_tokens_list_batch = []\n",
    "    block_token_ids_list_batch = []\n",
    "    \n",
    "    for (code, tokens, block_tokens_list, block_token_ids_list), target in batch:\n",
    "        target_batch.append(target)\n",
    "        code_batch.append(code)\n",
    "        tokens_batch.append(tokens)\n",
    "        block_tokens_list_batch.append(block_tokens_list)\n",
    "        block_token_ids_list_batch.append(block_token_ids_list)\n",
    "        \n",
    "    target_batch = torch.stack(target_batch)\n",
    "    \n",
    "    return (code_batch, tokens_batch, block_tokens_list_batch, block_token_ids_list_batch), target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn = dataset_collate_fn)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=True, collate_fn = dataset_collate_fn)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=True, collate_fn = dataset_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dataloader_train))\n",
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(code, tokens, block_tokens_list, block_token_ids_list), target = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(block_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(block_token_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "block_token_ids = block_token_ids_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(block_token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "void kvm_inject_x86_mce(CPUState *cenv, int bank, uint64_t status,\n",
      "\n",
      "                        uint64_t mcg_status, uint64_t addr, uint64_t misc,\n",
      "\n",
      "                        int abort_on_error)\n",
      "\n",
      "{\n",
      "\n",
      "#ifdef KVM_CAP_MCE\n",
      "\n",
      "    struct kvm_x86_mce mce = {\n",
      "\n",
      "        .bank = bank,\n",
      "\n",
      "        .status = status,\n",
      "\n",
      "        .mcg_status = mcg_status,\n",
      "\n",
      "        .addr = addr,\n",
      "\n",
      "        .misc = misc,\n",
      "\n",
      "    };\n",
      "\n",
      "    struct kvm_x86_mce_data data = {\n",
      "\n",
      "            .env = cenv,\n",
      "\n",
      "            .mce = &mce,\n",
      "\n",
      "    };\n",
      "\n",
      "\n",
      "\n",
      "    if (!cenv->mcg_cap) {\n",
      "\n",
      "        fprintf(stderr, \"MCE support is not enabled!\\n\");\n",
      "\n",
      "        return;\n",
      "\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "    run_on_cpu(cenv, kvm_do_inject_x86_mce, &data);\n",
      "\n",
      "#else\n",
      "\n",
      "    if (abort_on_error)\n",
      "\n",
      "        abort();\n",
      "\n",
      "#endif\n",
      "\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#code\n",
    "print(code[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading pre-trained code-understandable model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use CodeBERT or Transformer-XL to convert source code into embeddings, since they both are traiend on source code dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "embed_model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 47908, 449, 38486, 1215, 179, 21517, 1215, 1178, 5334, 1215, 119, 1755, 1640, 47378, 13360, 1009, 438, 41124, 6, 6979, 827, 6, 49315, 4027, 1215, 90, 2194, 6, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 49315, 4027, 1215, 90, 44355, 571, 1215, 29552, 6, 49315, 4027, 1215, 90, 49649, 6, 49315, 4027, 1215, 90, 29526, 6, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 6979, 34771, 1215, 261, 1215, 44223, 43, 50118, 50118, 45152, 50118, 50118, 10431, 1594, 9232, 229, 20954, 1215, 28494, 1215, 448, 8041, 50140, 1437, 1437, 1437, 29916, 449, 38486, 1215, 1178, 5334, 1215, 119, 1755, 475, 1755, 5457, 25522, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 479, 5760, 5457, 827, 6, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 479, 29552, 5457, 2194, 6, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 479, 29297, 571, 1215, 29552, 5457, 44355, 571, 1215, 29552, 6, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 479, 49439, 5457, 49649, 6, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 479, 45321, 5457, 29526, 6, 50140, 1437, 1437, 1437, 48855, 50140, 1437, 1437, 1437, 29916, 449, 38486, 1215, 1178, 5334, 1215, 119, 1755, 1215, 23687, 414, 5457, 25522, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 479, 41124, 5457, 740, 41124, 6, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 479, 119, 1755, 5457, 359, 119, 1755, 6, 50140, 1437, 1437, 1437, 48855, 50140, 50140, 1437, 1437, 1437, 114, 48209, 438, 41124, 46613, 29297, 571, 1215, 10906, 43, 25522, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 856, 49775, 1640, 620, 3624, 338, 6, 22, 448, 8041, 323, 16, 45, 9778, 328, 37457, 282, 45751, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 671, 131, 50140, 1437, 1437, 1437, 35524, 50140, 50140, 1437, 1437, 1437, 422, 1215, 261, 1215, 49345, 1640, 438, 41124, 6, 449, 38486, 1215, 5016, 1215, 179, 21517, 1215, 1178, 5334, 1215, 119, 1755, 6, 359, 23687, 4397, 50118, 50118, 10431, 44617, 50140, 1437, 1437, 1437, 114, 36, 873, 2723, 1215, 261, 1215, 44223, 43, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 34771, 47006, 50118, 50118, 10431, 49741, 50118, 50118, 24303, 50118, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 398])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_ids = block_token_ids_list[0][0]\n",
    "print(tokens_ids)\n",
    "tokens_ids = torch.tensor(tokens_ids)\n",
    "tokens_ids = tokens_ids.unsqueeze(0) #into a batch\n",
    "tokens_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = embed_model(tokens_ids)#[0]\n",
    "\n",
    "embed = outputs.last_hidden_state[:, 0, :] #CLS position.\n",
    "embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building code defect prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "\n",
    "class CodeDefectPredictionModel(nn.Module):\n",
    "    def __init__(self, device = torch.device(\"cpu\")):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "    \n",
    "        self.embed_model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "        #correlation prediction head.\n",
    "        self.pred_head = nn.Sequential(\n",
    "                    #nn.Dropout(p = 0.1),\n",
    "            \n",
    "                    #Since we add all representations as final context.\n",
    "                    #so we must re-center the representations statistically.\n",
    "                    #for NLP task, we DONT use batchnorm. instead SHOULD use layernorm.\n",
    "                    nn.LayerNorm(normalized_shape = (768)), \n",
    "                    nn.Linear(in_features = 768, out_features = 100, bias = True),\n",
    "                    nn.ReLU(),\n",
    "            \n",
    "                    nn.Linear(in_features = 100, out_features = 1, bias = True),\n",
    "                    nn.Sigmoid(),\n",
    "                    )\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        self.embed_model.eval()\n",
    "        \n",
    "        #A batch format:\n",
    "        #(code, tokens, block_tokens_list, block_token_ids_list), target\n",
    "\n",
    "        #get embeddings\n",
    "        embed_batch = []\n",
    "        \n",
    "        (code_batch, tokens_batch, block_tokens_list_batch, block_token_ids_list_batch), target = batch\n",
    "        \n",
    "        for block_token_ids_list in block_token_ids_list_batch:\n",
    "            #Compute each code block respectively.\n",
    "            block_embed_list = []\n",
    "            for tokens_ids in block_token_ids_list:\n",
    "                tokens_ids = torch.tensor(tokens_ids)\n",
    "                tokens_ids = tokens_ids.unsqueeze(0) #into a batch\n",
    "                tokens_ids = tokens_ids.to(self.device)\n",
    "                \n",
    "                # We only need last layer output the CLS position as embedding.!\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.embed_model(tokens_ids)\n",
    "                    #According to BERT paper, we cal [CLS] (token position 0) as \n",
    "                    #context representation vector.\n",
    "                    embed = outputs.last_hidden_state[:, 0, :] #CLS position.\n",
    "                    embed = embed.squeeze(0) #from 1x768 to 768\n",
    "                    block_embed_list.append(embed)\n",
    "            \n",
    "            block_embed = torch.stack(block_embed_list)\n",
    "            #simply add all embeddings to form the code embedding!\n",
    "            embed = torch.sum(block_embed, dim = 0)\n",
    "    \n",
    "            embed_batch.append(embed)\n",
    "        \n",
    "        \n",
    "        embed_batch = torch.stack(embed_batch)        \n",
    "        \n",
    "        #print(\"embed_batch.shape = \", embed_batch.shape)\n",
    "        \n",
    "        #stop gradients.\n",
    "        embed_batch = embed_batch.detach()\n",
    "        \n",
    "        probs_batch = self.pred_head(embed_batch)\n",
    "        probs_batch = probs_batch.squeeze(1)\n",
    "        \n",
    "        return probs_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#testing the model simply.\n",
    "model = CodeDefectPredictionModel(device)\n",
    "model.to(device)\n",
    "batch = next(iter(dataloader_train))\n",
    "probs = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4883, 0.4809, 0.4813, 0.4857, 0.4789, 0.4774, 0.4810, 0.4797, 0.4848,\n",
       "        0.4814], device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(probs >= 0.5, dtype = torch.float).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, labels = batch\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6948, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "#sending to GPU.\n",
    "probs = probs.to(device)\n",
    "labels = labels.float().to(device)\n",
    "\n",
    "criterion(probs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#from IPython.display import display, clear_output\n",
    "from IPython import display\n",
    "\n",
    "def train_one_epoch(\n",
    "          model, \n",
    "          device, \n",
    "          dataloader, \n",
    "          optimizer, \n",
    "          criterion,\n",
    "          epoch,\n",
    "          max_batches = None):\n",
    "    \n",
    "    # Enable gradient computing\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    if max_batches is None:\n",
    "        max_batches = len(dataloader)\n",
    "    \n",
    "    #some statistics\n",
    "    \n",
    "    #averaged loss in current epoch.\n",
    "    epoch_loss = 0.0\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    #accuracy in current epoch\n",
    "    batch_accuracy = 0.0\n",
    "    #accuracy in current batch\n",
    "    epoch_accuracy = 0.0\n",
    "    \n",
    "    #how many samples predicted correct.\n",
    "    epoch_corrects = 0.0\n",
    "    #how many samples trained in this epoch\n",
    "    epoch_total = 0.0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader, 1):\n",
    "        \n",
    "        (code_batch, tokens_batch, block_tokens_list_batch, block_token_ids_list_batch), labels_ = batch\n",
    "        \n",
    "        labels = labels_.float()\n",
    "        #sending labels to GPU if possible\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #predictions.\n",
    "        preds = model(batch)\n",
    "        \n",
    "        #computing BCE\n",
    "        loss = criterion(preds, labels)\n",
    "           \n",
    "        #computing gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        #optimizing the classifier, Notice: the GPT is fixed.\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        #computing accuracy in a batch\n",
    "        # torch.max() returns values, indices\n",
    "        preds_ = (preds >= 0.5).int().cpu().data\n",
    "        #batch_accuracy = torch.mean(preds > 0.5, dtype = torch.float).detach().cpu()\n",
    "        #batch_accuracy = torch.mean(preds_.float()).detach().cpu()\n",
    "        \n",
    "        #computing the total loss and average loss in one epoch\n",
    "        total_loss += loss.detach().cpu().numpy()\n",
    "        epoch_loss = total_loss / batch_idx\n",
    "        \n",
    "        #computing the correct and total samples\n",
    "        batch_corrects = torch.sum(labels_.cpu().data == preds_, dtype = torch.int)\n",
    "        batch_accuracy = batch_corrects / len(labels_)\n",
    "        epoch_corrects += batch_corrects\n",
    "        epoch_total += len(labels_)\n",
    "        epoch_accuracy = epoch_corrects / epoch_total         \n",
    "\n",
    "        #Updating training displays.\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        display.display('Epoch {} [{}/{} ({:.0f}%)]'.format(\n",
    "                    epoch, batch_idx, \n",
    "                    len(dataloader), \n",
    "                    100. * (batch_idx / len(dataloader))))\n",
    "        \n",
    "        display.display('* batch accuracy {:.2f}% epoch accuracy {:.2f}%'.format(\n",
    "                    100. * batch_accuracy, 100. * epoch_accuracy))\n",
    "        \n",
    "        display.display('* batch loss {:.6f} epoch loss {:.6f}'.format(\n",
    "                    loss.item(), epoch_loss))\n",
    "        display.display('* batch_corrects {}'.format(batch_corrects))\n",
    "        \n",
    "        if batch_idx > max_batches:\n",
    "            break\n",
    "    \n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = CodeDefectPredictionModel(device = device)\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True, \n",
    "                                               collate_fn = dataset_collate_fn)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "#for language models, Adam is a good option.The learning rate \n",
    "#typically less than 0.001 for stabability.\n",
    "optimizer = torch.optim.AdamW(\n",
    "                        model.pred_head.parameters(), \n",
    "                        lr = learning_rate,\n",
    "                        #momentum = 0.9, \n",
    "                        #weight_decay = 5e-4\n",
    "                      )\n",
    "\n",
    "#Loss function\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Epoch 1 [100/110 (91%)]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* batch accuracy 61.00% epoch accuracy 57.02%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* batch loss 0.666603 epoch loss 0.677467'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* batch_corrects 122'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_loss, epoch_accuracy = train_one_epoch(\n",
    "          model, \n",
    "          device, \n",
    "          dataloader_train, \n",
    "          optimizer, \n",
    "          criterion,\n",
    "          epoch = 1,\n",
    "          max_batches = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model(model, model_path):\n",
    "    \n",
    "    save_path = os.path.normpath(os.path.dirname(model_path)).rstrip(os.path.sep)\n",
    "        \n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    print(\"Save model weights to: \", model_path)\n",
    "    torch.save(model.state_dict(), model_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model weights to:  models/code_defect_detection_model.pth\n"
     ]
    }
   ],
   "source": [
    "save_model(model, \"models/code_defect_detection_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model_path, device):\n",
    "    model = CodeDefectPredictionModel(device)\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        #re-loading\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device)) \n",
    "        print(\"Loaded model weights from: \", model_path)\n",
    "    else:\n",
    "        print(\"Model weights not found.\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model weights from:  models/code_defect_detection_model.pth\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"models/code_defect_detection_model.pth\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from atomicwrites import atomic_write #we must guarantee the automicity of write operation.\n",
    "#import pickle\n",
    "\n",
    "def train(model, \n",
    "          device, \n",
    "          dataloader, \n",
    "          optimizer,\n",
    "          criterion,\n",
    "          epochs,\n",
    "          scheduler = None,\n",
    "          checkpoint = None):\n",
    "    \n",
    "    #if model_path is not None and not os.path.exists(model_path):\n",
    "    #    os.makedirs(model_path)\n",
    "    \n",
    "    loss_hist = []\n",
    "    accuracy_hist = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        \n",
    "        epoch_loss, epoch_accuracy = train_one_epoch(\n",
    "          model, \n",
    "          device, \n",
    "          dataloader, \n",
    "          optimizer, \n",
    "          criterion,\n",
    "          epoch,\n",
    "          max_batches = None)\n",
    "    \n",
    "        if scheduler:\n",
    "            #adjusting LR is necessary\n",
    "            scheduler.step()\n",
    "            \n",
    "        loss_hist.append(epoch_loss)\n",
    "        accuracy_hist.append(epoch_accuracy)\n",
    "   \n",
    "        if checkpoint is not None:\n",
    "            save_path = os.path.normpath(os.path.dirname(checkpoint)).rstrip(os.path.sep)\n",
    "        \n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path)\n",
    "        \n",
    "            #print(\"Save model weights to: \", checkpoint)\n",
    "            torch.save(model.state_dict(), checkpoint) \n",
    "            \n",
    "    return loss_hist, laccuracy_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True, \n",
    "                                               collate_fn = dataset_collate_fn)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "#for language models, Adam is a good option.The learning rate \n",
    "#typically less than 0.001 for stabability.\n",
    "optimizer = torch.optim.Adam(\n",
    "                        model.pred_head.parameters(), \n",
    "                        lr = learning_rate,\n",
    "                        #momentum = 0.9, \n",
    "                        #weight_decay = 5e-4\n",
    "                      )\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "#                                                step_size = 10, #dropping learning-rate every 10 steps. \n",
    "#                                                gamma = 0.1)\n",
    "scheduler = None\n",
    "\n",
    "#Loss function\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_hist, laccuracy_hist = train(model, \n",
    "          device, \n",
    "          dataloader_train, \n",
    "          optimizer,\n",
    "          criterion,\n",
    "          epochs = 50,\n",
    "          scheduler = scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(laccuracy_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, \n",
    "             device, \n",
    "             dataloader,\n",
    "             max_batches = None):\n",
    "    \n",
    "    # Disable gradient computing\n",
    "    model.eval()\n",
    "    \n",
    "    if max_batches is None:\n",
    "        max_batches = len(dataloader)\n",
    "    \n",
    "    batch_accuracy = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    \n",
    "    total_corrects = 0.0\n",
    "    total_entries = 0.0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader, 1):\n",
    "        \n",
    "        (code_batch, tokens_batch, block_tokens_list_batch, block_token_ids_list_batch), labels_ = batch\n",
    "        \n",
    "        \n",
    "        #no need to track gradients\n",
    "        with torch.no_grad():\n",
    "            preds = model(batch)\n",
    "                        \n",
    "        #computing accuracy in a batch\n",
    "        preds_ = (preds >= 0.5).int().cpu().data\n",
    "        \n",
    "        #computing the correct and total samples\n",
    "        batch_corrects = torch.sum(labels.cpu().data == preds_, dtype = torch.int)\n",
    "        batch_accuracy = batch_corrects / len(labels)\n",
    "        \n",
    "        total_corrects += batch_corrects\n",
    "        total_entries += len(labels)\n",
    "        total_accuracy = total_corrects / total_entries         \n",
    "\n",
    "        #Updating training displays.\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        display.display('Evaluation: [{}/{} ({:.0f}%)]'.format(\n",
    "                    batch_idx, \n",
    "                    len(dataloader), \n",
    "                    100. * (batch_idx / len(dataloader))))\n",
    "        \n",
    "        display.display('* batch accuracy {:.2f}% total accuracy {:.2f}%'.format(\n",
    "                    100. * batch_accuracy, 100. * total_accuracy))\n",
    "        \n",
    "        display.display('* total_corrects {} total_entries {}'.format(total_corrects, total_entries))\n",
    "        \n",
    "        if batch_idx >= max_batches:\n",
    "            break\n",
    "                \n",
    "    return total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Evaluation: [2/43 (5%)]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* batch accuracy 56.25% total accuracy 50.00%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* total_corrects 64.0 total_entries 128.0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m      3\u001b[0m dataloader_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset_test, \n\u001b[1;32m      4\u001b[0m                                                batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \n\u001b[1;32m      5\u001b[0m                                                shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m      6\u001b[0m                                                collate_fn \u001b[38;5;241m=\u001b[39m dataset_collate_fn)\n\u001b[0;32m----> 8\u001b[0m total_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdataloader_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m             \u001b[49m\u001b[43mmax_batches\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(total_accuracy)\n",
      "Cell \u001b[0;32mIn[133], line 25\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, device, dataloader, max_batches)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#no need to track gradients\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 25\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#computing accuracy in a batch\u001b[39;00m\n\u001b[1;32m     28\u001b[0m preds_ \u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[113], line 44\u001b[0m, in \u001b[0;36mCodeDefectPredictionModel.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     42\u001b[0m tokens_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(tokens_ids)\n\u001b[1;32m     43\u001b[0m tokens_ids \u001b[38;5;241m=\u001b[39m tokens_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m#into a batch\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m tokens_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokens_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# We only need last layer output the CLS position as embedding.!\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True, \n",
    "                                               collate_fn = dataset_collate_fn)\n",
    "\n",
    "total_accuracy = evaluate(model, \n",
    "             device, \n",
    "             dataloader_test,\n",
    "             max_batches = 10)\n",
    "\n",
    "print(total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True, \n",
    "                                               collate_fn = dataset_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(code_batch, tokens_batch, block_tokens_list_batch, block_token_ids_list_batch), labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5516, 0.4518, 0.4650, 0.5386, 0.4599, 0.4969, 0.5494, 0.5296, 0.4370,\n",
       "        0.5281, 0.4360, 0.4644, 0.4944, 0.4150, 0.5380, 0.4639, 0.4540, 0.5154,\n",
       "        0.4979, 0.4472, 0.3965, 0.4357, 0.4526, 0.5105, 0.4816, 0.5293, 0.5432,\n",
       "        0.5546, 0.4991, 0.5123, 0.5693, 0.5572, 0.5470, 0.5298, 0.5256, 0.5055,\n",
       "        0.4457, 0.5336, 0.4651, 0.4156, 0.5618, 0.4192, 0.5007, 0.5477, 0.4318,\n",
       "        0.5616, 0.5493, 0.5303, 0.4759, 0.5097, 0.5208, 0.6093, 0.4448, 0.4474,\n",
       "        0.4505, 0.4704, 0.4651, 0.5663, 0.5273, 0.4797, 0.4846, 0.4990, 0.5117,\n",
       "        0.4823, 0.5345, 0.4652, 0.4526, 0.4058, 0.3860, 0.5298, 0.5474, 0.4386,\n",
       "        0.4566, 0.5452, 0.5097, 0.4501, 0.4545, 0.4192, 0.4863, 0.4925, 0.4404,\n",
       "        0.4630, 0.5202, 0.4838, 0.4471, 0.4613, 0.4261, 0.5766, 0.4899, 0.5026,\n",
       "        0.5173, 0.5254, 0.5192, 0.5498, 0.4550, 0.5099, 0.4647, 0.4765, 0.4908,\n",
       "        0.4760], device='mps:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction probability.\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 0, 0, 1])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "        0, 0, 0, 0], device='mps:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds > 0.5).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5299999713897705"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#average accuracy\n",
    "torch.mean((preds > 0.5).int().cpu() == labels.int().cpu(), dtype = torch.float).cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
