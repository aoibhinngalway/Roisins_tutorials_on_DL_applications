{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd61e3a2-d8fe-4b7d-adc7-9c3658bd3f18",
   "metadata": {},
   "source": [
    "# A simple tutorial for QA correlation prediction with pre-trained GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cc0afbd-0275-4135-90d9-affca9f16b8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Jiaolin Luo (Róisín), Colm O'Riordan (supervisor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "444981b0-eeae-47e7-bf92-77e9e53fdb75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1100cb-6c53-40fd-a364-ec1ed1466df0",
   "metadata": {},
   "source": [
    "# GPU acceleration just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c7ff4056-2a2c-4448-a1de-9de3875d7cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hwacc_device_v3():\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        \n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print('CUDA memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    \n",
    "        device = torch.device('cuda')\n",
    "    # MacOS\n",
    "    elif hasattr(torch, \"backends\") and \\\n",
    "          hasattr(torch.backends, \"mps\") and \\\n",
    "          torch.backends.mps.is_available():\n",
    "                \n",
    "        device = torch.device('mps')\n",
    " \n",
    "    print(\"GPU device is: \", device)\n",
    "    \n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "efac3d72-77eb-40a8-9136-3eb15be29aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device is:  mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_hwacc_device_v3()\n",
    "#device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8978b8bf-3c5c-42db-b7f9-b06b6ab39990",
   "metadata": {},
   "source": [
    "# Loading math QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9e856198-fb66-4ad4-b213-a0e119fa4229",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36046"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import list_datasets\n",
    "\n",
    "datasets_list = list_datasets()\n",
    "len(datasets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a79e3676-26e6-40be-8d18-6d06eafa8c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iapp_wiki_qa_squad\n",
      "wiki_qa\n",
      "wiki_qa_ar\n",
      "wannaphong/iapp_wiki_qa_squad_oa\n",
      "sedthh/cmu_wiki_qa\n",
      "michaelthwan/wiki_qa_bart_1000row\n",
      "michaelthwan/wiki_qa_bart_10000row\n",
      "michaelthwan/oa_wiki_qa_bart_10000row\n"
     ]
    }
   ],
   "source": [
    "for ds in datasets_list:\n",
    "    if \"wiki_qa\" in ds:\n",
    "        print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0d218aa5-c807-477a-b510-28a3e672c537",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d424fc8be2494af49c356c0b6cd67ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55ee4d4ff294a95995111b52ca4c008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc27bbca3b9d4b5b8932ee9aceb80107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/13.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wiki_qa/default to /Users/roisinjiaolinluo/Documents/Research/AI_Research/question_answer_prediction/../../Dataset_Collection/wiki_qa/default/0.1.0/d2d236b5cbdc6fbdab45d168b4d678a002e06ddea3525733a24558150585951c...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46496fdfd0544e0be1b216d95b9f7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/6165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/20360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wiki_qa downloaded and prepared to /Users/roisinjiaolinluo/Documents/Research/AI_Research/question_answer_prediction/../../Dataset_Collection/wiki_qa/default/0.1.0/d2d236b5cbdc6fbdab45d168b4d678a002e06ddea3525733a24558150585951c. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01fab802e704464e8799e8f7533f9311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "qa_dataset = load_dataset(path = \"wiki_qa\",\n",
    "                       cache_dir = \"..\" + os.sep + \"..\" + os.sep + \"Dataset_Collection\", \n",
    "                       download_mode = \"reuse_dataset_if_exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2e6412-da05-4e52-a284-f629b46c20d7",
   "metadata": {},
   "source": [
    "## Investigating dataset shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0e01d0d4-1d7e-479e-bd36-91bb14a9750c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 6165\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 2733\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 20360\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "824df8ce-8384-49d5-8ec1-1c8143d90e7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': 'Q1',\n",
       " 'question': 'how are glacier caves formed?',\n",
       " 'document_title': 'Glacier cave',\n",
       " 'answer': 'A partly submerged glacier cave on Perito Moreno Glacier .',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c18937d2-3fc1-4633-b46c-4186f15136b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20360"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "106ab89c-c96e-45ea-a81b-142d97357065",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average age of students of an adult school is 40 years. 120 new students whose average age is 32 years joined the school. as a result the average age is decreased by 4 years. find the number of students of the school after joining of the new students . Write a short snippet of python function which solve this problem. No need to explain the answer, let the code output the answer.\n"
     ]
    }
   ],
   "source": [
    "print(qa_dataset['train'][0]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "76df8ac9-5f54-48d9-811c-3516a5cb1de1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A partly submerged glacier cave on Perito Moreno Glacier .\n"
     ]
    }
   ],
   "source": [
    "print(qa_dataset['train'][0]['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33caf4d-6c4f-4116-98b6-14300bb84a6e",
   "metadata": {},
   "source": [
    "## Setting dataset and splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3d29df82-4ce5-4347-90e6-b9e4d3b1274a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Setting format to torch or tensorflow\n",
    "qa_dataset.set_format(type='torch', columns=['question', 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2d468883-0bd8-4ecc-ad92-93539f2a011b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_dataset_train_ = qa_dataset['train']\n",
    "qa_dataset_val_ = qa_dataset['validation']\n",
    "qa_dataset_test_ = qa_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce05379-3322-40b3-83c7-7764662b8c9e",
   "metadata": {},
   "source": [
    "# Wrapping the dataset with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "03c9faee-71b6-4985-9f67-ace7ca702e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20360"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_dataset_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9b4eced6-7684-4192-8014-4278c0a58893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAPredictionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 dataset, \n",
    "                 paired_sampling_prob = 0.5, \n",
    "                 random_seed = 42):\n",
    "        self.dataset = dataset\n",
    "        self.paired_sampling_prob = paired_sampling_prob\n",
    "        self.dataset_size = len(dataset)\n",
    "        \n",
    "        self.dataset_indices = list(np.arange(0, self.dataset_size))\n",
    "        \n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        q = self.dataset[index]['question']\n",
    "        a = self.dataset[index]['answer']\n",
    "        \n",
    "        MAX_LEN=512\n",
    "        #Truncate Q and A to MAX_LEN\n",
    "        #TODO.\n",
    "        \n",
    "        if np.random.rand() < self.paired_sampling_prob:\n",
    "            y = 1\n",
    "        else:\n",
    "            y = 0\n",
    "            #Resampling a 'answer'\n",
    "            new_index = int(np.random.choice(self.dataset_indices))\n",
    "            a = self.dataset[new_index]['answer']\n",
    "        \n",
    "        return (q, a), y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "41e48390-0fef-4940-9037-52a9d02fb9fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_dataset_train = QAPredictionDataset(qa_dataset_train_, paired_sampling_prob = 0.5)\n",
    "qa_dataset_val = QAPredictionDataset(qa_dataset_val_, paired_sampling_prob = 0.5)\n",
    "qa_dataset_test = QAPredictionDataset(qa_dataset_test_, paired_sampling_prob = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d15680e9-a9fb-472a-b9f5-e6d8a1610389",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  how are glacier caves formed?\n",
      "A:  A partly submerged glacier cave on Perito Moreno Glacier .\n",
      "Paired prob:  1\n",
      "\n",
      "Q:  how are glacier caves formed?\n",
      "A:  In modern politics, the most high profile political campaigns are focused on candidates for head of state or head of government , often a President or Prime Minister .\n",
      "Paired prob:  0\n",
      "\n",
      "Q:  how are glacier caves formed?\n",
      "A:  Recovery was an international success and was named the best selling album of 2010 worldwide, joining The Eminem Show, which was the best seller of 2002.\n",
      "Paired prob:  0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    (q,a), y = qa_dataset_train[i]\n",
    "    print(\"Q: \", q)\n",
    "    print(\"A: \", a)\n",
    "    print(\"Paired prob: \", y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "eca153a1-9ac7-456c-8d66-28ad6ea59c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c0a62618-d586-4e83-8c2d-95b2a90386ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader_train = torch.utils.data.DataLoader(qa_dataset_train, batch_size=batch_size)\n",
    "dataloader_val = torch.utils.data.DataLoader(qa_dataset_val, batch_size=batch_size)\n",
    "dataloader_test = torch.utils.data.DataLoader(qa_dataset_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9ea1e1dc-395c-47b6-9df0-d91f1b20db66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dataloader_train))\n",
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f6bf4a08-24f3-4b6e-86ae-77189d7e4db8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(q,a),y = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c8621ae7-f033-48da-ba36-0c650dbdbe75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "dd05be21-9725-44f3-9919-882e37416170",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d1be76d5-04a0-467f-a076-84a4f2dd1b17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are glacier caves formed?\n"
     ]
    }
   ],
   "source": [
    "print(q[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "72330e5a-72c3-4b7a-aa34-d62b6d3407ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A partly submerged glacier cave on Perito Moreno Glacier .\n"
     ]
    }
   ],
   "source": [
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3652c1c8-4d58-43f1-9968-911bd7622225",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b7cd17-8874-43d2-9be5-b708625e1c80",
   "metadata": {},
   "source": [
    "# Loading pre-trained OpenAI GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "02023a77-a341-44c1-8e83-886a2ac0784b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import OpenAIGPTTokenizer, OpenAIGPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6abb5d5b-7321-4bf0-819c-c40fee00ed5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "gpt = OpenAIGPTModel.from_pretrained('openai-gpt')\n",
    "_ = gpt.eval() #Freezing GPT model to make it not trainabale.\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042bf7e0-9014-41e2-8629-db081adac467",
   "metadata": {},
   "source": [
    "## Testing GPT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7bc1b03b-9bae-48b0-a220-edd1e0feae57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text =  how are glacier caves formed?\n",
      "{'input_ids': tensor([[  718,   640, 25397, 11464,  2768,   257]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Tokenized input\n",
    "text = q[0]\n",
    "print(\"text = \", text)\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0195e089-bbcc-4c74-86ab-8f824f0d303f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sending GPT to GPU if possible\n",
    "gpt.to(device)\n",
    "\n",
    "#sending inputs tensors to GPU if possible\n",
    "inputs.to(device)\n",
    "\n",
    "#get embeddings.\n",
    "with torch.no_grad():\n",
    "    outputs = gpt(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "618c1a82-f6fa-47e5-aa6c-c9a769256ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b5010e4e-924b-4366-a17c-08d177ef0f21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "fa3dd2ba-608d-45db-8467-765e756c4ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "35e7c8b4-4444-4e9c-a06b-13f74274d6ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We use this position as embedding.\n",
    "last_hidden_states[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f34c8dd-3478-4342-a997-648c7f8f57c0",
   "metadata": {},
   "source": [
    "# Building GPT based contextualized QA correlation prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "4eaa25bd-d02a-42bc-a48f-1c1e792c041f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import OpenAIGPTTokenizer, OpenAIGPTModel\n",
    "\n",
    "class QACorrelationPredictionModel(nn.Module):\n",
    "    def __init__(self, device = torch.device(\"cpu\")):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        #Pre-trained Contextual Embedding\n",
    "        self.gpt = OpenAIGPTModel.from_pretrained('openai-gpt')\n",
    "        #self.gpt.to(device)\n",
    "        #self.gpt.eval() #freezing model not trainable.\n",
    "        \n",
    "        #GPT tokenizer\n",
    "        self.tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "\n",
    "        #correlation prediction head.\n",
    "        self.pred_head = nn.Sequential(\n",
    "                    #nn.Dropout(p = 0.3),\n",
    "                    #nn.LayerNorm(normalized_shape = (768*2)), #for NLP task, we DONT use batchnorm. instead SHOULD use layernorm.\n",
    "                    nn.Linear(in_features = 768 * 2, out_features = 300, bias = True),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(in_features = 300, out_features = 1, bias = True),\n",
    "                    nn.Sigmoid(),\n",
    "                    )\n",
    "        #self.pred_head.to(device)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        self.gpt.eval()\n",
    "        \n",
    "        #A batch consists of ((q,a), y), in which\n",
    "        #q and q are lists, y an array.\n",
    "\n",
    "        #get embeddings\n",
    "        qa_embed = []\n",
    "        \n",
    "        (q_list, a_list), y = batch\n",
    "        \n",
    "        for q, a in zip(q_list, a_list):\n",
    "            \n",
    "            # Tokenized Q and A\n",
    "            tokenized_q = tokenizer.tokenize(q)\n",
    "            tokenized_a = tokenizer.tokenize(a)\n",
    "\n",
    "            inputs_q = self.tokenizer(q, return_tensors=\"pt\")\n",
    "            inputs_a = self.tokenizer(a, return_tensors=\"pt\")\n",
    "            \n",
    "            #sending tensors to GPU if possible\n",
    "            inputs_q = inputs_q.to(self.device)\n",
    "            inputs_a = inputs_a.to(self.device)\n",
    "            \n",
    "            # We only need last layer output the CLS position as embedding.!\n",
    "            with torch.no_grad():\n",
    "                outputs_q = self.gpt(**inputs_q)\n",
    "                outputs_a = self.gpt(**inputs_a)\n",
    "      \n",
    "            q_embed = outputs_q.last_hidden_state[0][0]\n",
    "            a_embed = outputs_q.last_hidden_state[0][0]\n",
    "        \n",
    "            #Concatenate two 768 into 768*2\n",
    "            embed = torch.cat([q_embed, a_embed])\n",
    "            qa_embed.append(embed)\n",
    "            \n",
    "        qa_embed = torch.stack(qa_embed)\n",
    "        #print(\"qa_embed.shape = \", qa_embed.shape)\n",
    "        \n",
    "        logits = self.pred_head(qa_embed)\n",
    "        logits = logits.squeeze(1)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "95b25720-d943-45a1-8261-ea9246567d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "2336024c-a65b-40ea-b75d-ef50edca111b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q,a),labels = batch\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "77444dc8-0058-4fab-b440-355c1ea11f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "#testing the model simply.\n",
    "model = QACorrelationPredictionModel(device = device)\n",
    "model.to(device)\n",
    "batch = next(iter(dataloader_train))\n",
    "probs = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "6251de52-8749-4251-b136-b2f1c2072ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4502, 0.4502, 0.4502, 0.4502, 0.4502, 0.4502, 0.4502, 0.4502, 0.4502,\n",
       "        0.4502, 0.4502, 0.4502, 0.4502, 0.4502, 0.4502, 0.4502, 0.4502, 0.4502,\n",
       "        0.4502, 0.4502, 0.4502, 0.4502, 0.4502, 0.4502, 0.4502, 0.4502, 0.4502,\n",
       "        0.4502, 0.4502, 0.4502, 0.4502, 0.4502], device='mps:0',\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "df4cb7c0-89d2-48e3-9803-cb020f83075f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "2d198337-ada1-4c9f-9d3b-f050a9bd75c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False], device='mps:0')"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "c6c047fd-2913-4e32-baeb-042a9007c99d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(probs >= 0.5, dtype = torch.float).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "181f5ff2-a7d3-47ad-80b9-1ca24b7d9d12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(_, _), labels = batch\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "2a2cd870-5455-4c25-aa5d-9d4ab9d72cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "7990bd6c-e5db-47e4-9452-0f36cc7d381d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "757bee96-a099-4f77-94f7-4786f5e49336",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6981, device='mps:0', grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "#sending to GPU.\n",
    "probs = probs.to(device)\n",
    "labels = labels.float().to(device)\n",
    "\n",
    "criterion(probs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd427fa-a78d-4d6c-9103-0793f72199ea",
   "metadata": {},
   "source": [
    "# Training on epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "9ded1e1c-b804-4db8-845e-5c4914bbbcc0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#from IPython.display import display, clear_output\n",
    "from IPython import display\n",
    "\n",
    "def train_one_epoch(\n",
    "          model, \n",
    "          device, \n",
    "          dataloader, \n",
    "          optimizer, \n",
    "          criterion,\n",
    "          epoch,\n",
    "          max_batches = None):\n",
    "    \n",
    "    # Enable gradient computing\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    if max_batches is None:\n",
    "        max_batches = len(dataloader)\n",
    "    \n",
    "    #some statistics\n",
    "    \n",
    "    #averaged loss in current epoch.\n",
    "    epoch_loss = 0.0\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    #accuracy in current epoch\n",
    "    batch_accuracy = 0.0\n",
    "    #accuracy in current batch\n",
    "    epoch_accuracy = 0.0\n",
    "    \n",
    "    #how many samples predicted correct.\n",
    "    epoch_corrects = 0.0\n",
    "    #how many samples trained in this epoch\n",
    "    epoch_total = 0.0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader, 1):\n",
    "        \n",
    "        (q,a), labels_ = batch\n",
    "        \n",
    "        labels = labels_.float()\n",
    "        #sending labels to GPU if possible\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #predictions.\n",
    "        preds = model(batch)\n",
    "        \n",
    "        #computing BCE\n",
    "        loss = criterion(preds, labels)\n",
    "           \n",
    "        #computing gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        #optimizing the classifier, Notice: the GPT is fixed.\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        #computing accuracy in a batch\n",
    "        # torch.max() returns values, indices\n",
    "        preds_ = (preds >= 0.5).int().cpu().data\n",
    "        #batch_accuracy = torch.mean(preds > 0.5, dtype = torch.float).detach().cpu()\n",
    "        #batch_accuracy = torch.mean(preds_.float()).detach().cpu()\n",
    "        \n",
    "        #computing the total loss and average loss in one epoch\n",
    "        total_loss += loss.detach().cpu().numpy()\n",
    "        epoch_loss = total_loss / batch_idx\n",
    "        \n",
    "        #computing the correct and total samples\n",
    "        batch_corrects = torch.sum(labels_.cpu().data == preds_, dtype = torch.int)\n",
    "        batch_accuracy = batch_corrects / len(labels_)\n",
    "        epoch_corrects += batch_corrects\n",
    "        epoch_total += len(labels_)\n",
    "        epoch_accuracy = epoch_corrects / epoch_total         \n",
    "\n",
    "        #Updating training displays.\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        display.display('Epoch {} [{}/{} ({:.0f}%)]'.format(\n",
    "                    epoch, batch_idx, \n",
    "                    len(dataloader), \n",
    "                    100. * (batch_idx / len(dataloader))))\n",
    "        \n",
    "        display.display('* batch accuracy {:.2f}% epoch accuracy {:.2f}%'.format(\n",
    "                    100. * batch_accuracy, 100. * epoch_accuracy))\n",
    "        \n",
    "        display.display('* loss {:.6f} epoch loss {:.6f}'.format(\n",
    "                    loss.item(), epoch_loss))\n",
    "        display.display('* batch_corrects {}'.format(batch_corrects))\n",
    "        \n",
    "        if batch_idx > max_batches:\n",
    "            break\n",
    "    \n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "15f77096-c2c2-4b3a-85f7-7061cd688abd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QACorrelationPredictionModel(\n",
       "  (gpt): OpenAIGPTModel(\n",
       "    (tokens_embed): Embedding(40478, 768)\n",
       "    (positions_embed): Embedding(512, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pred_head): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=300, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = QACorrelationPredictionModel(device = device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "8ad05155-dc6f-4c5b-b02e-ad6f188914ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Epoch 1 [28/637 (4%)]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* batch accuracy 37.50% epoch accuracy 52.23%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* loss 0.733202 epoch loss 0.834022'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* batch_corrects 12'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[280], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#Loss function\u001b[39;00m\n\u001b[1;32m     15\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m epoch_loss, epoch_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m          \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmax_batches\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[273], line 49\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, device, dataloader, optimizer, criterion, epoch, max_batches)\u001b[0m\n\u001b[1;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#predictions.\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#computing BCE\u001b[39;00m\n\u001b[1;32m     52\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(preds, labels)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[262], line 55\u001b[0m, in \u001b[0;36mQACorrelationPredictionModel.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# We only need last layer output the CLS position as embedding.!\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 55\u001b[0m     outputs_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs_q\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     outputs_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs_a)\n\u001b[1;32m     58\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m outputs_q\u001b[38;5;241m.\u001b[39mlast_hidden_state[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/transformers/models/openai/modeling_openai.py:504\u001b[0m, in \u001b[0;36mOpenAIGPTModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    502\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m--> 504\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/transformers/models/openai/modeling_openai.py:260\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    252\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[1;32m    253\u001b[0m     x,\n\u001b[1;32m    254\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    255\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    256\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    257\u001b[0m )\n\u001b[1;32m    258\u001b[0m a \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 260\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(n)\n\u001b[1;32m    262\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(n \u001b[38;5;241m+\u001b[39m m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1494\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1491\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1495\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(qa_dataset_train, batch_size=batch_size)\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "                        model.pred_head.parameters(), \n",
    "                        lr = learning_rate,\n",
    "                        #momentum = 0.9, \n",
    "                        #weight_decay = 5e-4\n",
    "                      )\n",
    "\n",
    "#Loss function\n",
    "criterion = torch.nn.BCELoss(reduction='mean')\n",
    "\n",
    "epoch_loss, epoch_accuracy = train_one_epoch(\n",
    "          model, \n",
    "          device, \n",
    "          dataloader_train, \n",
    "          optimizer, \n",
    "          criterion,\n",
    "          epoch = 1,\n",
    "          max_batches = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f62a4-5166-402a-b90c-17557b68395c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
